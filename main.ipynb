{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T14:14:13.098372Z",
     "start_time": "2025-07-31T14:14:07.485619Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import torch\n",
    "from torchvision.transforms import Compose, Resize, ToTensor\n",
    "from torchvision import transforms\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fffc14c5a44f2ff4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T13:46:34.488734Z",
     "start_time": "2025-07-31T13:46:34.366204Z"
    }
   },
   "outputs": [],
   "source": [
    "model = YOLO(\"yolo11n-seg.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8639aeab3792a656",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T13:48:27.977857Z",
     "start_time": "2025-07-31T13:48:25.541463Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 18 apples, 155.6ms\n",
      "Speed: 4.7ms preprocess, 155.6ms inference, 28.7ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    }
   ],
   "source": [
    "import cv2 as cv\n",
    "img = cv.imread(\"dataset/test.jpg\")\n",
    "results = model(img)[0]\n",
    "results.show()  # display to screen\n",
    "results.save(filename=\"result.jpg\")\n",
    "# cv.imshow(\"Display window\", img)\n",
    "k = cv.waitKey(0) # Wait for a keystroke in the window"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "!pip install timm torch ultralytics",
   "id": "5f46b6d715f73e4f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model_type = \"DPT_Large\"\n",
    "midas = torch.hub.load(\"intel-isl/MiDaS\", model_type)"
   ],
   "id": "cd6c323c649358d1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "midas.to(device)\n",
    "midas.eval()"
   ],
   "id": "30b2909daa303456"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "custom_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((384, 384)),  # or any size multiple of 32\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "img = cv2.imread(\"dataset/test.jpg\")\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "input_tensor = custom_transform(img).unsqueeze(0).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    prediction = midas(input_tensor)\n",
    "\n",
    "# Resize prediction to original image size\n",
    "prediction = torch.nn.functional.interpolate(\n",
    "    prediction.unsqueeze(1),\n",
    "    size=(448,640),\n",
    "    mode=\"bicubic\",\n",
    "    align_corners=False\n",
    ").squeeze()\n",
    "depth_map = prediction.cpu().numpy()\n",
    "plt.imshow(depth_map)"
   ],
   "id": "f7eb2fd1ca35d4ff"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Get fruit items detecions",
   "id": "901ea6b87d992d41"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_index(results, results_2):\n",
    "  index = []\n",
    "  import numpy as np\n",
    "  array_index = np.zeros(len(results[0]['boxes']), dtype=bool)\n",
    "  for i in range(len(results_2[0].boxes.xyxy)):\n",
    "    #segmentation\n",
    "    x1,y1,x2,y2 = results_2[0].boxes.xyxy[i]\n",
    "    for y in range(len(results[0]['boxes'])):\n",
    "      # if array_index[y] == True:\n",
    "      #   break\n",
    "      x1_1,y1_1,x2_1,y2_1 = results[0]['boxes'][y]\n",
    "      if abs(x1-x1_1) < 50 and abs(x2-x2_1) < 50 and abs(y1-y1_1) < 50 and abs(y2-y2_1) < 50:\n",
    "        index.append(i)\n",
    "        # array_index[y] = True\n",
    "      # else:\n",
    "      #   break\n",
    "  return index"
   ],
   "id": "80765e70de2c4ddc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def draw_image_segmentation(results_2,index):\n",
    "  img_bgr = results_2[0].orig_img.copy()\n",
    "  img = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "  mask_ids = index\n",
    "\n",
    "  H, W = img.shape[:2]\n",
    "  combined_mask = np.zeros((H, W), dtype=np.uint8)\n",
    "\n",
    "  for idx in mask_ids:\n",
    "      m = results_2[0].masks.data[idx].cpu().numpy().astype(np.uint8)\n",
    "      if m.shape != (H, W):\n",
    "          m = cv2.resize(m, (W, H), interpolation=cv2.INTER_NEAREST)\n",
    "      combined_mask = np.logical_or(combined_mask, m)\n",
    "\n",
    "  overlay = img.copy()\n",
    "  color = np.array([0, 255, 0], dtype=np.uint8)\n",
    "  alpha = 0.5\n",
    "  overlay[combined_mask] = (alpha * color + (1 - alpha) * overlay[combined_mask]).astype(np.uint8)\n",
    "\n",
    "  plt.imshow(overlay)\n",
    "  plt.axis(\"off\")\n",
    "  plt.show()\n"
   ],
   "id": "11ddc07b2ecf5e24"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Load the model to do detection",
   "id": "3bbce2709c25f9da"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from google.colab import userdata\n",
    "from huggingface_hub import login\n",
    "login(token=userdata.get(\"HF_TOKEN\"))"
   ],
   "id": "c0204f9418e9ad78"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import requests\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection\n",
    "\n",
    "model_id = \"IDEA-Research/grounding-dino-base\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "model_dec = AutoModelForZeroShotObjectDetection.from_pretrained(model_id).to(device)"
   ],
   "id": "11698411b20dc8e8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "import torch\n",
    "\n",
    "def show_gd_results(img, results, score_thr=0.25):\n",
    "    if isinstance(img, str):\n",
    "        img = Image.open(img).convert(\"RGB\")\n",
    "    elif isinstance(img, torch.Tensor):\n",
    "        if img.ndim == 3 and img.shape[0] in (1,3):\n",
    "            img = img.permute(1,2,0).detach().cpu().numpy()\n",
    "        else:\n",
    "            img = img.detach().cpu().numpy()\n",
    "        img = (img*255).astype(np.uint8) if img.max()<=1 else img.astype(np.uint8)\n",
    "        img = Image.fromarray(img)\n",
    "    elif not isinstance(img, Image.Image):\n",
    "        img = Image.fromarray(img)\n",
    "\n",
    "    r = results[0] if isinstance(results, (list, tuple)) else results\n",
    "    boxes  = r[\"boxes\"].detach().cpu().numpy()\n",
    "    scores = r[\"scores\"].detach().cpu().numpy()\n",
    "    labels = r.get(\"labels\", r.get(\"text_labels\", []))\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8,8))\n",
    "    ax.imshow(np.asarray(img))\n",
    "    for (x1,y1,x2,y2), s, lab in zip(boxes, scores, labels):\n",
    "        if s < score_thr:\n",
    "            continue\n",
    "        ax.add_patch(Rectangle((x1, y1), x2-x1, y2-y1, fill=False, linewidth=2, edgecolor='blue'))\n",
    "        ax.text(x1, max(0, y1-5), f\"{lab} {s:.2f}\",\n",
    "                fontsize=9, color=\"white\",\n",
    "                bbox=dict(facecolor=\"blue\", alpha=0.5, pad=2))\n",
    "    ax.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "# Example:\n",
    "# show_gd_results(original_pil_image, results, score_thr=0.3)\n"
   ],
   "id": "56d93406f277a4be"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# image_url = \"/content/test.jpg\"\n",
    "def get_detection(image_path, model_dec):\n",
    "  image = Image.open(image_path).convert(\"RGB\")\n",
    "  # Check for cats and remote controls\n",
    "  # VERY important: text queries need to be lowercased + end with a dot\n",
    "  text = \"banana . broccoli . avocado . tomato . onion . apple .\"\n",
    "\n",
    "  inputs = processor(images=image, text=text, return_tensors=\"pt\").to(device)\n",
    "  with torch.no_grad():\n",
    "      outputs = model_dec(**inputs)\n",
    "  results = processor.post_process_grounded_object_detection(\n",
    "      outputs,\n",
    "      inputs.input_ids,\n",
    "      target_sizes=[image.size[::-1]]\n",
    "  )\n",
    "  show_gd_results(image, results)\n",
    "  return results"
   ],
   "id": "4cb0b54c2b291c0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Crop image",
   "id": "1775d28db4ac242c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "def get_index_detection(results):\n",
    "  dic_ind = {\"banana\" : [], \"tomato\" : [] , \"broccoli\" : [], \"onion\" : [], \"apple\" : [], \"avocado\" : []}\n",
    "  for index in range(len(results[0]['text_labels'])):\n",
    "    if results[0]['text_labels'][index] not in dic_ind:\n",
    "      continue\n",
    "    dic_ind[results[0]['text_labels'][index]].append(index)\n",
    "\n",
    "  xyxy = torch.zeros((6, 4), dtype= torch.float32)  # 3 rows, 2 columns\n",
    "  index_arr = 0\n",
    "  for fruit, index in dic_ind.items():\n",
    "    x1, y1, x2,y2 = results[0]['boxes'][dic_ind[fruit][0]]\n",
    "    xyxy[index_arr][0] = x1\n",
    "    xyxy[index_arr][1] = y1\n",
    "    xyxy[index_arr][2] = x2\n",
    "    xyxy[index_arr][3] = y2\n",
    "    index_arr += 1\n",
    "\n",
    "\n",
    "  index_arr = 0\n",
    "  for fruit, index in dic_ind.items():\n",
    "    for i in index:\n",
    "      x1, y1, x2,y2 = results[0]['boxes'][i]\n",
    "      if (x1 < xyxy[index_arr][0]):\n",
    "        xyxy[index_arr][0] = x1\n",
    "\n",
    "      if (y1 < xyxy[index_arr][1]):\n",
    "        xyxy[index_arr][1] = y1\n",
    "\n",
    "      if (x2 > xyxy[index_arr][2]):\n",
    "        xyxy[index_arr][2] = x2\n",
    "\n",
    "      if (y2 > xyxy[index_arr][3]):\n",
    "        xyxy[index_arr][3] = y2\n",
    "    index_arr += 1\n",
    "  return dic_ind,xyxy"
   ],
   "id": "9ca623081f906fb6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def draw_xyxy_on_image(img_path, xyxy, classes, seen=None, out_path=None):\n",
    "    img = cv2.imread(img_path)\n",
    "    H, W = img.shape[:2]\n",
    "\n",
    "    boxes = xyxy.detach().to('cpu').round().to(torch.int64).numpy()\n",
    "\n",
    "    # optional clamp\n",
    "    boxes[:, [0,2]] = boxes[:, [0,2]].clip(0, W)\n",
    "    boxes[:, [1,3]] = boxes[:, [1,3]].clip(0, H)\n",
    "\n",
    "    for i, (x1, y1, x2, y2) in enumerate(boxes):\n",
    "        if seen is not None and (not bool(seen[i].item())):\n",
    "            continue\n",
    "        if x2 <= x1 or y2 <= y1:\n",
    "            continue\n",
    "        cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 3)\n",
    "        label = classes[i]\n",
    "        cv2.putText(img, label, (x1, max(0, y1 - 8)),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "    if out_path:\n",
    "        cv2.imwrite(out_path, img)\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n"
   ],
   "id": "7c8c36c8e6f666cc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import cv2\n",
    "import torch\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def crop_xyxy_regions(img_path, xyxy, classes, seen=None, save_dir=\"crops\", show=True):\n",
    "    names = []\n",
    "    crops = []\n",
    "    img = cv2.imread(img_path)\n",
    "    H, W = img.shape[:2]\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    boxes = xyxy.detach().to('cpu').round().to(torch.int64).numpy()\n",
    "\n",
    "    for i, (x1, y1, x2, y2) in enumerate(boxes):\n",
    "        if seen is not None and not bool(seen[i].item()):\n",
    "            continue\n",
    "        if x2 <= x1 or y2 <= y1:\n",
    "            continue\n",
    "        x1 = max(0, min(W, x1))\n",
    "        x2 = max(0, min(W, x2))\n",
    "        y1 = max(0, min(H, y1))\n",
    "        y2 = max(0, min(H, y2))\n",
    "\n",
    "        crop = img[y1:y2, x1:x2]\n",
    "        filename = f\"{classes[i]}_crop.jpg\"\n",
    "        names.append(filename)\n",
    "        crops.append(crop)\n",
    "\n",
    "        cv2.imwrite(os.path.join(save_dir, filename), crop)\n",
    "        print(f\"Saved {filename}\")\n",
    "\n",
    "    if show and crops:\n",
    "        cols = min(3, len(crops))\n",
    "        rows = (len(crops) + cols - 1) // cols\n",
    "        plt.figure(figsize=(5*cols, 5*rows))\n",
    "        for j, crop in enumerate(crops):\n",
    "            plt.subplot(rows, cols, j+1)\n",
    "            plt.imshow(cv2.cvtColor(crop, cv2.COLOR_BGR2RGB))\n",
    "            plt.title(names[j])\n",
    "            plt.axis(\"off\")\n",
    "        plt.show()\n",
    "\n",
    "    return names, crops\n"
   ],
   "id": "8bf7ffadaac8e50e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Segmentation the whole image",
   "id": "c3b8a5ba70a1828a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from ultralytics import SAM\n",
    "\n",
    "# Load a model\n",
    "# For SAM=sam_b.pt, SAM2=sam2_b.pt, SAM2.1=sam2.1_b.pt\n",
    "model = SAM(\"sam2.1_l.pt\")\n",
    "\n",
    "model.info()  # Display model information (optional)"
   ],
   "id": "5abc56fde71ae2d4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Run inference (image or video)\n",
    "def get_whole_photo_seg(image_path, model):\n",
    "  results_2 = model.predict(image_path, conf = 0.1, iou = 1, batch = 10, max_det = 600, rect = False)  # image\n",
    "  # results = model(\"https://youtu.be/LNwODJXcvt4\")  # video file\n",
    "\n",
    "  results_2[0].show()  # Display results\n",
    "  return results_2"
   ],
   "id": "9d5a24a967eaf046"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "bd593c90aeda8312"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
